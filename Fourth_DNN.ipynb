{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fourth_DNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neethipoonacha/EIP/blob/master/Fourth_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoNexnpOHzjx",
        "colab_type": "text"
      },
      "source": [
        "install keras pip - package management system from anaconda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "6e520743-4c71-4ff2-d33c-79254f3e7f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiLICPymH9Ug",
        "colab_type": "text"
      },
      "source": [
        "import libraries and modules import numpy - for reproducing results from the result set import Sequential - for feed forward of linear stack of neural network layers import Flatten - core layers of keras import Convolution2D - convolutional layers that will help efficiently train on image data import np_utils - utitlities to transform data import mnist - libraries to import datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.layers import Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFU29RcjIDxY",
        "colab_type": "text"
      },
      "source": [
        "Load image data from MNIST data from - https://s3.amazonaws.com/img-datasets/mnist.npz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A2MpDTFIJX2",
        "colab_type": "text"
      },
      "source": [
        "There are 6000 samples with 28*28 resolution each .The first to fifth sample is plotted here\n",
        "As we can see for all images that egdges are blank and do not count .So the images fall in the (5,25) range"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "778b8cf8-fbc2-44fd-9cf1-294188591a1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])\n",
        "plt.imshow(X_train[1])\n",
        "plt.imshow(X_train[2])\n",
        "plt.imshow(X_train[3])\n",
        "plt.imshow(X_train[4])\n",
        "plt.imshow(X_train[5])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f379dbd3198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCIrwA0ERfiAowg8ERfiBoP4ffm+Zwo6qf/gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y78P7kQsIPIA",
        "colab_type": "text"
      },
      "source": [
        "Preprocess input data for keras RGB images have a depth of 3 . minst images have a depth of 1 . explictly mentioning to process the data and to transform dataset from having shape (n, width, height) to (n, depth, width, height)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld3Y2X3yITHN",
        "colab_type": "text"
      },
      "source": [
        "convert our data type to float32 and normalize our data values to the range [0, 1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGYT3ng6IXLJ",
        "colab_type": "text"
      },
      "source": [
        "The 10 different classes are represented as a single 1-dimensional array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "15e7b6e9-f3d0-42a1-ac0d-0936d2ca46eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXbiLu1kIan_",
        "colab_type": "text"
      },
      "source": [
        "The y_train and y_test data are split into 10 distinct class labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "540199ce-7156-4cce-a7fd-1caa83aebfb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfFn8uwDIgKm",
        "colab_type": "text"
      },
      "source": [
        "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1))) The input shape parameter should be the shape of 1 sample corresponding to the (depth, width, height) of each digit image here it is 28*28\n",
        "The first 3 params(32, 3, 3) represent the no of filters = 32 the size of the kernel(no of rows and no of col) =3, 3 step size is (1,1) by default.\n",
        "After this add more layers to the module MaxPooling2D = to reduce the number of parameters in our model by sliding a 2x2 pooling filter across the previous layer and taking the max of the 4 values in the 2x2 filter Flatten = flattened convolution layers to 1-dimensional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "459cf030-07f5-482b-b5e4-1360109e63b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "from keras.layers import Activation, MaxPooling2D\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(10, 3, 3, activation='relu', input_shape=(28,28,1)))#26\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(10, 3, 3, activation='relu'))  #24\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu')) #22\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #11\n",
        "\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu'))#9\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(10, 3, 3, activation='relu'))#7\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(10, 1, activation='relu'))#7\n",
        "model.add(Convolution2D(10, 7))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAesDcE2QEP4",
        "colab_type": "text"
      },
      "source": [
        "Restricted to 13K params by adding batchnormaisation after each convolution layer and adding maxpooling after edge detection\n",
        "\n",
        "\n",
        "\n",
        "Tried to stay at 15k Params by the below \n",
        "\n",
        "\n",
        "\n",
        "name         size                      parameters\n",
        "---  --------  -------------------------    ------------------------\n",
        "0\tinput\t\t\t\t\t1x28x28\t\t\t\t\t0\n",
        "1\tconv2d1   (28-(3-1))=26 10*26*26\t\t(3*3*1)*10  \t\t=  100\n",
        "batchnormalisation \n",
        "2\tconv2d1   (26-(3-1))=24 20*24*24        (3*3*10)*10         = 900\n",
        "batchnormalisation \n",
        "3 conv2d1   (24-(3-1))=22 30*22*22        (3*3*10) *20        = 1800 \n",
        "batchnormalisation \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdAYg1k9K7Z",
        "colab_type": "code",
        "outputId": "848e827e-583c-45b5-bed2-140a129a91d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_263 (Conv2D)          (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "batch_normalization_223 (Bat (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_264 (Conv2D)          (None, 24, 24, 10)        910       \n",
            "_________________________________________________________________\n",
            "batch_normalization_224 (Bat (None, 24, 24, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_265 (Conv2D)          (None, 22, 22, 20)        1820      \n",
            "_________________________________________________________________\n",
            "batch_normalization_225 (Bat (None, 22, 22, 20)        80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_36 (MaxPooling (None, 11, 11, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_266 (Conv2D)          (None, 9, 9, 20)          3620      \n",
            "_________________________________________________________________\n",
            "batch_normalization_226 (Bat (None, 9, 9, 20)          80        \n",
            "_________________________________________________________________\n",
            "conv2d_267 (Conv2D)          (None, 7, 7, 10)          1810      \n",
            "_________________________________________________________________\n",
            "batch_normalization_227 (Bat (None, 7, 7, 10)          40        \n",
            "_________________________________________________________________\n",
            "conv2d_268 (Conv2D)          (None, 7, 7, 10)          110       \n",
            "_________________________________________________________________\n",
            "conv2d_269 (Conv2D)          (None, 1, 1, 10)          4910      \n",
            "_________________________________________________________________\n",
            "flatten_36 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 13,560\n",
            "Trainable params: 13,420\n",
            "Non-trainable params: 140\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0GoEpBlIktG",
        "colab_type": "text"
      },
      "source": [
        "compile and run the model with loss function categorical_crossentropy and optimizer = adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EJIudehbpAx",
        "colab_type": "text"
      },
      "source": [
        "The change here is to \n",
        "\n",
        "-   Adding callback to set the learning rate scheduled as per the progress per epoch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1411
        },
        "outputId": "ed04b979-39ed-46b0-da9d-6df262882b6f"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.0344 - acc: 0.9889 - val_loss: 0.0371 - val_acc: 0.9894\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 0.0224 - acc: 0.9929 - val_loss: 0.0335 - val_acc: 0.9892\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 10s 164us/step - loss: 0.0168 - acc: 0.9944 - val_loss: 0.0345 - val_acc: 0.9900\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 10s 164us/step - loss: 0.0130 - acc: 0.9957 - val_loss: 0.0333 - val_acc: 0.9905\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0095 - acc: 0.9969 - val_loss: 0.0407 - val_acc: 0.9883\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 0.0076 - acc: 0.9974 - val_loss: 0.0374 - val_acc: 0.9896\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 0.0062 - acc: 0.9980 - val_loss: 0.0295 - val_acc: 0.9915\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.0286 - val_acc: 0.9924\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0309 - val_acc: 0.9919\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0309 - val_acc: 0.9922\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 10s 166us/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0332 - val_acc: 0.9917\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 10s 164us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0310 - val_acc: 0.9931\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0348 - val_acc: 0.9920\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0354 - val_acc: 0.9926\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 10s 166us/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0340 - val_acc: 0.9920\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 8.5225e-04 - acc: 0.9998 - val_loss: 0.0335 - val_acc: 0.9933\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 5.2021e-04 - acc: 1.0000 - val_loss: 0.0336 - val_acc: 0.9925\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 10s 167us/step - loss: 6.8141e-04 - acc: 0.9999 - val_loss: 0.0361 - val_acc: 0.9923\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 10s 164us/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0361 - val_acc: 0.9919\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 6.0725e-04 - acc: 0.9999 - val_loss: 0.0347 - val_acc: 0.9931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f373dc79438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtugU7bVIrPx",
        "colab_type": "text"
      },
      "source": [
        "Fit the training data by adding the batch size ( create 64 batches with each batch having one feature each) and run it for 10 epochs(iterations of back forth of entire data set)\n",
        "\n",
        "Batch size increased to 64\n",
        "Epochs increased to 30\n",
        "\n",
        "Increasing the batch size increases the number of classes in each epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3drzF0qIsV8",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the model on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiqsozkqIvl4",
        "colab_type": "text"
      },
      "source": [
        "print the accuracy of prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "534e2740-99e8-4b1b-9102-d811df2b99ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.03385149991993555, 0.9935]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTbYkTNLIxtN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "3d188e06-1237-45c1-d481-a06fd1408fec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.22055195e-14 3.64051669e-15 2.09077089e-09 1.05266444e-08\n",
            "  3.32061162e-16 3.42179085e-14 3.15399350e-24 1.00000000e+00\n",
            "  2.54245908e-12 7.90756738e-10]\n",
            " [1.46266710e-09 5.23258260e-12 1.00000000e+00 3.16496044e-17\n",
            "  3.64782733e-16 1.09991023e-18 2.84261519e-13 6.85450891e-17\n",
            "  1.77549964e-14 7.27784315e-19]\n",
            " [1.85546529e-08 9.99990821e-01 6.76190481e-08 7.01369856e-07\n",
            "  2.13387366e-07 3.91330310e-08 5.14877696e-09 6.86768817e-06\n",
            "  6.73876741e-08 1.22196661e-06]\n",
            " [9.99999523e-01 4.14929635e-15 7.04568189e-15 2.84538587e-13\n",
            "  4.75375924e-13 6.57758743e-12 4.92824938e-07 1.38975298e-11\n",
            "  1.34983934e-11 1.11062340e-10]\n",
            " [3.24480349e-16 2.35930572e-16 1.64661503e-12 3.17572086e-19\n",
            "  1.00000000e+00 8.78780933e-17 1.60537775e-13 1.96616019e-15\n",
            "  8.24111504e-13 1.18850574e-08]\n",
            " [8.73960886e-08 9.99976993e-01 1.45292290e-07 1.87749993e-09\n",
            "  7.71120625e-08 5.29368416e-09 9.08083220e-10 2.17154611e-05\n",
            "  1.00406288e-08 9.10959102e-07]\n",
            " [1.81952484e-22 1.66721595e-12 1.39253509e-09 5.85757441e-12\n",
            "  9.99999523e-01 7.94769795e-13 2.29477633e-21 1.03374045e-11\n",
            "  7.75208164e-08 4.35372158e-07]\n",
            " [4.43007208e-20 2.06899675e-19 1.73922349e-10 2.21999458e-14\n",
            "  2.91200966e-08 1.44307629e-19 6.09009911e-28 1.54029850e-15\n",
            "  6.58077823e-08 9.99999881e-01]\n",
            " [1.86560017e-10 5.50087623e-12 4.58744726e-05 2.54799827e-11\n",
            "  1.37410673e-11 9.96722281e-01 3.22930422e-03 1.99059964e-15\n",
            "  2.65092854e-06 5.20821857e-14]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GY4Upv4dsUR",
        "colab_type": "code",
        "outputId": "080ec437-d080-4104-e93c-6bdc33aac092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_14'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    else:\n",
        "        plot_x, plot_y = 1, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0, 0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-19229f66b51a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filter %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mplot_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mvis_img_in_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-19229f66b51a>\u001b[0m in \u001b[0;36mvis_img_in_filter\u001b[0;34m(img, layer_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n\u001b[1;32m     23\u001b[0m                       layer_name = 'conv2d_14'):\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mimg_ascs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilter_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'conv2d_14'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tvptcn8dxvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}